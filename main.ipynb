{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9621d89e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import nltk\n",
    "import json\n",
    "import codecs\n",
    "import csv\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51f8ad",
   "metadata": {},
   "source": [
    "Readine the lines and the conversation texts to extract text for training the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5690a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the movie_lines file and econding used for this file is utf-8, read function helps in reading the file and \n",
    "#split function formats the data\n",
    "lines = open('movie_lines.txt',encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "#Reading the conversations file\n",
    "conv_lines = open('movie_conversations.txt',encoding='utf-8',errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef8d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the id and the conversation from the movie_lines text file.\n",
    "# Step 1 - Splitting the file on the indicator '+++$+++'\n",
    "# Step 2 - Extracting the 5th section of each line as that depicts  the conversation and storing the convesation against the ID\n",
    "#          present in the first section of the sentence\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]\n",
    "\n",
    "# Conversation lines data \n",
    "# Step 1: Splitting each line on the identifier\n",
    "# Step 2: Extracting the last segement of the sentence and then replace quotation marks and in between spaces\n",
    "conv=[]\n",
    "for line in conv_lines:\n",
    "    _conv = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    conv.append(_conv.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a521ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L194 Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "L195 Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "L196 Not the hacking and gagging and spitting part.  Please.\n",
      "L197 Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n"
     ]
    }
   ],
   "source": [
    "#A sample conversation -\n",
    "#u16 +++$+++ u25 +++$+++ m1 +++$+++ ['L2256', 'L2257', 'L2258', 'L2259', 'L2260'] this is the 250th row from the input\n",
    "for i in conv[0]:\n",
    "    print(i, id2line[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909f798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a pair of conversation and breaking it into input and response pair, ny drawing the lines from the id2line dict\n",
    "pairs=[]\n",
    "for cnv in conv:\n",
    "    for i in range(len(cnv)-1):\n",
    "        inp=id2line[cnv[i]].strip()\n",
    "        res=id2line[cnv[i+1]].strip()\n",
    "        if inp and res:\n",
    "            pairs.append([inp,res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ab654f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
       "  \"Well, I thought we'd start with pronunciation, if that's okay with you.\"],\n",
       " [\"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       "  'Not the hacking and gagging and spitting part.  Please.'],\n",
       " ['Not the hacking and gagging and spitting part.  Please.',\n",
       "  \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"],\n",
       " [\"You're asking me out.  That's so cute. What's your name again?\",\n",
       "  'Forget it.'],\n",
       " [\"No, no, it's my fault -- we didn't have a proper introduction ---\",\n",
       "  'Cameron.'],\n",
       " ['Cameron.',\n",
       "  \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\"],\n",
       " [\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\",\n",
       "  'Seems like she could get a date easy enough...'],\n",
       " ['Why?',\n",
       "  'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.'],\n",
       " ['Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n",
       "  \"That's a shame.\"],\n",
       " ['Gosh, if only we could find Kat a boyfriend...',\n",
       "  'Let me see what I can do.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e9bc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing inputs and response pair in a text file\n",
    "file_name='conv_formatted.txt'\n",
    "delimiter = '\\t'\n",
    "# Unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "with open(file_name,'w',encoding='utf-8') as out_file:\n",
    "    writer=csv.writer(out_file,delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in pairs:\n",
    "        writer.writerow(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013df364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the formatted conversation file\n",
    "line_fmt = open(file_name,encoding='utf-8').read().strip().split('\\n')       #Strip removes the leading and trailing characters\n",
    "line_fmt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fee79b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load total 65049 pairs with length <= max_length (10)\n",
      "[['there .', 'where ?'], ['you have my word . as a gentleman', 'you are sweet .'], ['hi .', 'looks like things worked out tonight huh ?']]\n"
     ]
    }
   ],
   "source": [
    "#Some basic formatting of the data\n",
    "#Converting the data into lower-case, trim and remove all non-letter characters\n",
    "def NormalizeText(s):\n",
    "    s=s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "#filterPairs - Stripping sentences into smaller ones by setting a threshold limit on number of words, if either the input\n",
    "#or the response is less than threshold length, then it is added to the list of valid pairs, else skipped\n",
    "#'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.\n",
    "#Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.'\n",
    "#the above sentence pair is not considered as valid when the max_length is 10 as both the input & response's length > 10\n",
    "def filterPairs(pairs,max_length):\n",
    "    valid_pair=[]\n",
    "    for pair in pairs:\n",
    "        inp, resp = pair[0].split(' '),pair[1].split(' ')\n",
    "        if len(inp) < max_length and len(resp) < max_length:\n",
    "            valid_pair.append(pair)\n",
    "    print(f'load total {len(valid_pair)} pairs with length <= max_length (10)')\n",
    "    return valid_pair\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n",
    "#     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "pairs=[[NormalizeText(clean_text(s)) for s in l.split('\\t')] for l in line_fmt]\n",
    "valid_pairs=filterPairs(pairs,10)\n",
    "print(valid_pairs[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5436f",
   "metadata": {},
   "source": [
    "The first thing to do is to create values for our start of sentence, end of sentence, and sentence padding special tokens. When we tokenize text (split text into its atomic constituent pieces), we need special tokens to delineate both the beginning and end of a sentence, as well as to pad sentence (or some other text chunk) storage structures when sentences are shorter then the maximum allowable space. More on this later.\n",
    "\n",
    "PAD_token = 0   # Used for padding short sentences\n",
    "\n",
    "SOS_token = 1   # Start-of-sentence token\n",
    "\n",
    "EOS_token = 2   # End-of-sentence token\n",
    "\n",
    "word2index - assigns an index to each unique word\n",
    "\n",
    "word2count - Measures the frequency of each word in the corpus. Example - 'there': 2013, the word there appears 2013 times\n",
    "\n",
    "index2word - Used for retrieving the word after predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ccec8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token=0\n",
    "sos_token=1\n",
    "eos_token=2\n",
    "\n",
    "class Vocab:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.trimmed = False\n",
    "        self.word2count={}\n",
    "        self.word2index={}\n",
    "        self.index2word={pad_token:'PAD',sos_token:'SOS',eos_token:'EOS'}\n",
    "        self.numword=3\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addword(word)\n",
    "    \n",
    "    def addword(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word]=self.numword\n",
    "            self.word2count[word]=1\n",
    "            self.index2word[self.numword]=word\n",
    "            self.numword+=1\n",
    "        else:\n",
    "            self.word2count[word]+=1\n",
    "     \n",
    "    def trim(self,min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_word = []\n",
    "        for word in self.word2count:\n",
    "            if self.word2count[word] > min_count:\n",
    "                keep_word.append(word)\n",
    "        \n",
    "        #Dictionary is reinitialized to setup the word2index dictionary\n",
    "        self.word2count={}\n",
    "        self.word2index={}\n",
    "        self.index2word={pad_token:'PAD',sos_token:'SOS',eos_token:'EOS'}\n",
    "        self.numword=3\n",
    "        \n",
    "        for word in keep_word:\n",
    "            self.addword(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d77801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word in vocabulary is : 19477\n"
     ]
    }
   ],
   "source": [
    "voc=Vocab()\n",
    "for pair in valid_pairs:\n",
    "    voc.add_sentence(pair[0])\n",
    "    voc.add_sentence(pair[1])\n",
    "    \n",
    "print(f'total word in vocabulary is : {voc.numword}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf75b7a",
   "metadata": {},
   "source": [
    "Trimming the rare words from the input pairs so that the model can converge easily as these rare words hinder the convergence of the model\n",
    "\n",
    "Step 1 \n",
    "Remove all the words having min_count less than 3 from vocabulary\n",
    "\n",
    "Step 2\n",
    "Used to vocabulary to remove pair of words which contains word that is not part of the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9827d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_rareword(voc,valid_pairs,min_count):\n",
    "\n",
    "    voc.trim(min_count)     #Rebuilding the word2index dictionary to trim the rarewords\n",
    "    \n",
    "    trimmed_pair = []\n",
    "    for pair in valid_pairs:\n",
    "        inp_sent = pair[0]\n",
    "        res_sent = pair[1]\n",
    "        keep_input = True\n",
    "        keep_resp = True\n",
    "        \n",
    "        for word in inp_sent.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        \n",
    "        for word in res_sent.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_resp = False\n",
    "                break\n",
    "            \n",
    "        if keep_input & keep_resp:\n",
    "            trimmed_pair.append(pair)\n",
    "    print(f'the trimming process make the total {len(valid_pairs)} ==> {len(trimmed_pair)} trimmed pair)')\n",
    "    return voc,trimmed_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd699711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the trimming process make the total 65049 ==> 29857 trimmed pair)\n"
     ]
    }
   ],
   "source": [
    "voc,trimmed_pairs = trim_rareword(voc,valid_pairs,min_count = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6541aab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1544"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.numword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaadbed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in trimmed_pairs:\n",
    "    for word1,word2 in zip(pair[0].split(' '),pair[1].split(' ')):\n",
    "        if word1 not in voc.word2index:\n",
    "            print(word1)\n",
    "        if word2 not in voc.word2index:\n",
    "            print(word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddc804",
   "metadata": {},
   "source": [
    "Data Transform to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27224de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['there .', 'where ?'],\n",
       " ['you have my word . as a gentleman', 'you are sweet .'],\n",
       " ['hi .', 'looks like things worked out tonight huh ?'],\n",
       " ['well no . . .', 'then that is all you had to say .'],\n",
       " ['then that is all you had to say .', 'but'],\n",
       " ['what good stuff ?', 'the real you .']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed_pairs[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5926540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the trimmed_pairs to vector representatiom\n",
    "word2vec = [voc.word2index[word] for word in trimmed_pairs[5][0].split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aba9c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 48, 49, 6]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45953009",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in trimmed_pairs:\n",
    "    inp_cnt=0\n",
    "    resp_cnt=0\n",
    "    for word in pair[0].split(' '):\n",
    "        inp_cnt+=1\n",
    "    for word in pair[1].split(' '):\n",
    "        resp_cnt+=1\n",
    "    if inp_cnt > 10 and resp_cnt > 10:\n",
    "        print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aaeb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform1(voc,sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')]+[eos_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9206c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_line_length = 10\n",
    "encoder_inp=[]\n",
    "decoder_inp=[]\n",
    "for pair in trimmed_pairs:\n",
    "    encoder_inp.append(transform1(voc,pair[0]))\n",
    "    decoder_inp.append([sos_token]+transform1(voc,pair[1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e1079f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(voc, trimmed_pairs):\n",
    "\n",
    "    encoder_inp = np.zeros(shape=(len(trimmed_pairs), 20))\n",
    "    encoder_out = np.zeros(shape=(len(trimmed_pairs), 20))\n",
    "    idx=0\n",
    "    for pair in trimmed_pairs:\n",
    "        \n",
    "        j=0\n",
    "        for word1,word2 in zip(pair[0].split(' '),pair[1].split(' ')):\n",
    "            encoder_inp[idx][j]=voc.word2index[word1]\n",
    "            encoder_out[idx][j]=voc.word2index[word2]\n",
    "            j+=1\n",
    "        encoder_inp[idx][j]=eos_token\n",
    "        encoder_out[idx][j]=eos_token\n",
    "        idx+=1\n",
    "    \n",
    "    return encoder_inp,encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3eb67e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_input,encoder_output=transform(voc,trimmed_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42adc07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(encoder_input_data.shape,decoder_input_data.shape[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5922d45",
   "metadata": {},
   "source": [
    "First Model -\n",
    "1. Embedding Layer for drawing insight from the text data\n",
    "2. LSTM layer for Encoder\n",
    "3. LSTM layer for Decoder\n",
    "4. Dense Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46b430da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: KerasTensor(type_spec=TensorSpec(shape=(None, 512), dtype=tf.float32, name=None), name='lstm_1/PartitionedCall:0', description=\"created by layer 'lstm_1'\")\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = keras.layers.Input(shape=(None,),dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=(None,),dtype=np.int32)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    encoder_embeddings = keras.layers.Embedding(voc.numword, 128)(encoder_inputs)\n",
    "    decoder_embeddings = keras.layers.Embedding(voc.numword, 128)(decoder_inputs)    \n",
    "    \n",
    "    encoder = keras.layers.LSTM(512,return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "    encoder_state = [state_h, state_c]\n",
    "    \n",
    "    print(f'Encoder:',encoder_outputs)\n",
    "    \n",
    "    decoder_cell = keras.layers.LSTM(512,return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_cell(decoder_embeddings,initial_state=encoder_state)\n",
    "    output_layer = keras.layers.Dense(voc.numword,activation='softmax')\n",
    "    decoder_outputs = output_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4779b244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 128)    197632      ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 128)    197632      ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 512),        1312768     ['embedding_2[0][0]']            \n",
      "                                 (None, 512),                                                     \n",
      "                                 (None, 512)]                                                     \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, None, 512),  1312768     ['embedding_3[0][0]',            \n",
      "                                 (None, 512),                     'lstm_1[0][1]',                 \n",
      "                                 (None, 512)]                     'lstm_1[0][2]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 1544)   792072      ['lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,812,872\n",
      "Trainable params: 3,812,872\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    \n",
    "    encoder_input_data = pad_sequences(encoder_inp, maxlen=max_line_length+1, value=pad_token, padding='post')\n",
    "    decoder_input_data = pad_sequences(decoder_inp,maxlen=max_line_length+2,value=pad_token,padding='post')\n",
    "    decoder_target_data = np.zeros((len(decoder_input_data), max_line_length+2, voc.numword),dtype='float32')\n",
    "    \n",
    "    model = keras.Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "    model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['acc'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5befe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29857, 12, 1544) (29857, 11) (29857, 12)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_target_data.shape, encoder_input_data.shape, decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e4f4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_data = []\n",
    "for pair in trimmed_pairs:\n",
    "    resp_data.append(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "321d5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, target_seq in enumerate(resp_data):\n",
    "    for t, seq in enumerate(target_seq.split(' ')):\n",
    "        if t > 0:\n",
    "            decoder_target_data[i,t-1,voc.word2index[seq]]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01ce89cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "222/222 [==============================] - 10s 25ms/step - loss: 1.6262 - acc: 0.0673 - val_loss: 1.6610 - val_acc: 0.0631\n",
      "Epoch 2/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6235 - acc: 0.0676 - val_loss: 1.6424 - val_acc: 0.0631\n",
      "Epoch 3/40\n",
      "222/222 [==============================] - 4s 19ms/step - loss: 1.6178 - acc: 0.0676 - val_loss: 1.6407 - val_acc: 0.0631\n",
      "Epoch 4/40\n",
      "222/222 [==============================] - 4s 19ms/step - loss: 1.6156 - acc: 0.0675 - val_loss: 1.6387 - val_acc: 0.0631\n",
      "Epoch 5/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6161 - acc: 0.0676 - val_loss: 1.6476 - val_acc: 0.0631\n",
      "Epoch 6/40\n",
      "222/222 [==============================] - 4s 19ms/step - loss: 1.6152 - acc: 0.0676 - val_loss: 1.6460 - val_acc: 0.0631\n",
      "Epoch 7/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6144 - acc: 0.0676 - val_loss: 1.6433 - val_acc: 0.0631\n",
      "Epoch 8/40\n",
      "222/222 [==============================] - 4s 19ms/step - loss: 1.6148 - acc: 0.0676 - val_loss: 1.6373 - val_acc: 0.0631\n",
      "Epoch 9/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6155 - acc: 0.0676 - val_loss: 1.6343 - val_acc: 0.0631\n",
      "Epoch 10/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6142 - acc: 0.0676 - val_loss: 1.6360 - val_acc: 0.0631\n",
      "Epoch 11/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6155 - acc: 0.0676 - val_loss: 1.6343 - val_acc: 0.0631\n",
      "Epoch 12/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6149 - acc: 0.0676 - val_loss: 1.6356 - val_acc: 0.0631\n",
      "Epoch 13/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6147 - acc: 0.0675 - val_loss: 1.6369 - val_acc: 0.0631\n",
      "Epoch 14/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6149 - acc: 0.0676 - val_loss: 1.6522 - val_acc: 0.0631\n",
      "Epoch 15/40\n",
      "222/222 [==============================] - 4s 19ms/step - loss: 1.6152 - acc: 0.0676 - val_loss: 1.6486 - val_acc: 0.0631\n",
      "Epoch 16/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6154 - acc: 0.0675 - val_loss: 1.6445 - val_acc: 0.0631\n",
      "Epoch 17/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6151 - acc: 0.0673 - val_loss: 1.6422 - val_acc: 0.0631\n",
      "Epoch 18/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6155 - acc: 0.0676 - val_loss: 1.6416 - val_acc: 0.0631\n",
      "Epoch 19/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6151 - acc: 0.0676 - val_loss: 1.6449 - val_acc: 0.0631\n",
      "Epoch 20/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6150 - acc: 0.0676 - val_loss: 1.6381 - val_acc: 0.0631\n",
      "Epoch 21/40\n",
      "222/222 [==============================] - 4s 19ms/step - loss: 1.6154 - acc: 0.0676 - val_loss: 1.6357 - val_acc: 0.0631\n",
      "Epoch 22/40\n",
      "222/222 [==============================] - 4s 19ms/step - loss: 1.6152 - acc: 0.0676 - val_loss: 1.6489 - val_acc: 0.0631\n",
      "Epoch 23/40\n",
      "222/222 [==============================] - 4s 19ms/step - loss: 1.6153 - acc: 0.0674 - val_loss: 1.6431 - val_acc: 0.0631\n",
      "Epoch 24/40\n",
      "222/222 [==============================] - 4s 19ms/step - loss: 1.6139 - acc: 0.0676 - val_loss: 1.6356 - val_acc: 0.0631\n",
      "Epoch 25/40\n",
      "222/222 [==============================] - 4s 19ms/step - loss: 1.6146 - acc: 0.0676 - val_loss: 1.6441 - val_acc: 0.0631\n",
      "Epoch 26/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6149 - acc: 0.0676 - val_loss: 1.6382 - val_acc: 0.0631\n",
      "Epoch 27/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6149 - acc: 0.0676 - val_loss: 1.6433 - val_acc: 0.0631\n",
      "Epoch 28/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6153 - acc: 0.0676 - val_loss: 1.6489 - val_acc: 0.0631\n",
      "Epoch 29/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6151 - acc: 0.0676 - val_loss: 1.6392 - val_acc: 0.0631\n",
      "Epoch 30/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6150 - acc: 0.0676 - val_loss: 1.6434 - val_acc: 0.0631\n",
      "Epoch 31/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6154 - acc: 0.0676 - val_loss: 1.6353 - val_acc: 0.0631\n",
      "Epoch 32/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6154 - acc: 0.0676 - val_loss: 1.6422 - val_acc: 0.0631\n",
      "Epoch 33/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6147 - acc: 0.0676 - val_loss: 1.6375 - val_acc: 0.0631\n",
      "Epoch 34/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6147 - acc: 0.0676 - val_loss: 1.6505 - val_acc: 0.0631\n",
      "Epoch 35/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6149 - acc: 0.0676 - val_loss: 1.6458 - val_acc: 0.0631\n",
      "Epoch 36/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6149 - acc: 0.0676 - val_loss: 1.6506 - val_acc: 0.0631\n",
      "Epoch 37/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6161 - acc: 0.0676 - val_loss: 1.6425 - val_acc: 0.0631\n",
      "Epoch 38/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6145 - acc: 0.0676 - val_loss: 1.6477 - val_acc: 0.0631\n",
      "Epoch 39/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6157 - acc: 0.0676 - val_loss: 1.6448 - val_acc: 0.0631\n",
      "Epoch 40/40\n",
      "222/222 [==============================] - 4s 18ms/step - loss: 1.6157 - acc: 0.0676 - val_loss: 1.6402 - val_acc: 0.0631\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit([encoder_input_data,decoder_input_data],decoder_target_data,\n",
    "              batch_size=128,\n",
    "              epochs=40,\n",
    "              validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "543c084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:/Users/manju/OneDrive/Desktop/Ruparna/ChatBot Using RNN/movie_dialogue_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d96144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
