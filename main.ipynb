{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9621d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import nltk\n",
    "import json\n",
    "import codecs\n",
    "import csv\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51f8ad",
   "metadata": {},
   "source": [
    "Readine the lines and the conversation texts to extract text for training the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5690a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the movie_lines file and econding used for this file is utf-8, read function helps in reading the file and \n",
    "#split function formats the data\n",
    "lines = open('movie_lines.txt',encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "#Reading the conversations file\n",
    "conv_lines = open('movie_conversations.txt',encoding='utf-8',errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef8d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the id and the conversation from the movie_lines text file.\n",
    "# Step 1 - Splitting the file on the indicator '+++$+++'\n",
    "# Step 2 - Extracting the 5th section of each line as that depicts  the conversation and storing the convesation against the ID\n",
    "#          present in the first section of the sentence\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]\n",
    "\n",
    "# Conversation lines data \n",
    "# Step 1: Splitting each line on the identifier\n",
    "# Step 2: Extracting the last segement of the sentence and then replace quotation marks and in between spaces\n",
    "conv=[]\n",
    "for line in conv_lines:\n",
    "    _conv = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    conv.append(_conv.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a521ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L194 Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "L195 Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "L196 Not the hacking and gagging and spitting part.  Please.\n",
      "L197 Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n"
     ]
    }
   ],
   "source": [
    "#A sample conversation -\n",
    "#u16 +++$+++ u25 +++$+++ m1 +++$+++ ['L2256', 'L2257', 'L2258', 'L2259', 'L2260'] this is the 250th row from the input\n",
    "for i in conv[0]:\n",
    "    print(i, id2line[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "909f798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a pair of conversation and breaking it into input and response pair, ny drawing the lines from the id2line dict\n",
    "pairs=[]\n",
    "for cnv in conv:\n",
    "    for i in range(len(cnv)-1):\n",
    "        inp=id2line[cnv[i]].strip()\n",
    "        res=id2line[cnv[i+1]].strip()\n",
    "        if inp and res:\n",
    "            pairs.append([inp,res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4ab654f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
       "  \"Well, I thought we'd start with pronunciation, if that's okay with you.\"],\n",
       " [\"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       "  'Not the hacking and gagging and spitting part.  Please.'],\n",
       " ['Not the hacking and gagging and spitting part.  Please.',\n",
       "  \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"],\n",
       " [\"You're asking me out.  That's so cute. What's your name again?\",\n",
       "  'Forget it.'],\n",
       " [\"No, no, it's my fault -- we didn't have a proper introduction ---\",\n",
       "  'Cameron.'],\n",
       " ['Cameron.',\n",
       "  \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\"],\n",
       " [\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\",\n",
       "  'Seems like she could get a date easy enough...'],\n",
       " ['Why?',\n",
       "  'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.'],\n",
       " ['Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n",
       "  \"That's a shame.\"],\n",
       " ['Gosh, if only we could find Kat a boyfriend...',\n",
       "  'Let me see what I can do.']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e9bc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing inputs and response pair in a text file\n",
    "file_name='conv_formatted.txt'\n",
    "delimiter = '\\t'\n",
    "# Unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "with open(file_name,'w',encoding='utf-8') as out_file:\n",
    "    writer=csv.writer(out_file,delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in pairs:\n",
    "        writer.writerow(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "013df364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the formatted conversation file\n",
    "line_fmt = open(file_name,encoding='utf-8').read().strip().split('\\n')       #Strip removes the leading and trailing characters\n",
    "line_fmt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "59642a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load total 64265 pairs with length <= max_length (10)\n",
      "[['there .', 'where ?'], ['you have my word . as a gentleman', 'you re sweet .'], ['hi .', 'looks like things worked out tonight huh ?']]\n"
     ]
    }
   ],
   "source": [
    "#Some basic formatting of the data\n",
    "#Converting the data into lower-case, trim and remove all non-letter characters\n",
    "def NormalizeText(s):\n",
    "    s=s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "#filterPairs - Stripping sentences into smaller ones by setting a threshold limit on number of words, if either the input\n",
    "#or the response is less than threshold length, then it is added to the list of valid pairs, else skipped\n",
    "#'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.\n",
    "#Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.'\n",
    "#the above sentence pair is not considered as valid when the max_length is 10 as both the input & response's length > 10\n",
    "def filterPairs(pairs,max_length):\n",
    "    valid_pair=[]\n",
    "    for pair in pairs:\n",
    "        inp, resp = pair[0].split(' '),pair[1].split(' ')\n",
    "        if len(inp) < max_length and len(resp) < max_length:\n",
    "            valid_pair.append(pair)\n",
    "    print(f'load total {len(valid_pair)} pairs with length <= max_length (10)')\n",
    "    return valid_pair\n",
    "\n",
    "\n",
    "pairs=[[NormalizeText(s) for s in l.split('\\t')] for l in line_fmt]\n",
    "valid_pairs=filterPairs(pairs,10)\n",
    "print(valid_pairs[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c3fb8",
   "metadata": {},
   "source": [
    "The first thing to do is to create values for our start of sentence, end of sentence, and sentence padding special tokens. When we tokenize text (split text into its atomic constituent pieces), we need special tokens to delineate both the beginning and end of a sentence, as well as to pad sentence (or some other text chunk) storage structures when sentences are shorter then the maximum allowable space. More on this later.\n",
    "\n",
    "PAD_token = 0   # Used for padding short sentences\n",
    "\n",
    "SOS_token = 1   # Start-of-sentence token\n",
    "\n",
    "EOS_token = 2   # End-of-sentence token\n",
    "\n",
    "word2index - assigns an index to each unique word\n",
    "\n",
    "word2count - Measures the frequency of each word in the corpus. Example - 'there': 2013, the word there appears 2013 times\n",
    "\n",
    "index2word - Used for retrieving the word after predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e6ec2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token=0\n",
    "sos_token=1\n",
    "eos_token=2\n",
    "\n",
    "class Vocab:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2count={}\n",
    "        self.word2index={}\n",
    "        self.index2word={pad_token:'PAD',sos_token:'SOS',eos_token:'EOS'}\n",
    "        self.numword=3\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addword(word)\n",
    "    \n",
    "    def addword(self,word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word]=self.numword\n",
    "            self.word2count[word]=1\n",
    "            self.index2word[self.numword]=word\n",
    "            self.numword+=1\n",
    "        else:\n",
    "            self.word2count[word]+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5033352c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total word in vocabulary is : 17989\n"
     ]
    }
   ],
   "source": [
    "voc=Vocab()\n",
    "for pair in valid_pairs:\n",
    "    voc.add_sentence(pair[0])\n",
    "    voc.add_sentence(pair[1])\n",
    "    \n",
    "print(f'total word in vocabulary is : {voc.numword}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "87aa2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "for word in voc.word2count:\n",
    "    if  voc.word2count[word] < 3:\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bccb3ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10169\n"
     ]
    }
   ],
   "source": [
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c92a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
